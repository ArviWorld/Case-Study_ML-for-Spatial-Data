---
title:  "Land cover classification with ML models in R"
subtitle: "Predicting the invasive species 'Gorse (Ulex europaeus)' on the Banks Peninsula in New Zealand"
tutorial_by: "Dr. Hanna Meyer"
adapted_by: "Aravindh Venkatraman"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 6
  word_document: 
    toc: yes
    toc_depth: '6'
  html_document: default
  always_allow_html: yes
---

# Table of Contents

1.  [Acknowledgements](#acknowledgements)
2.  [Introduction](#introduction)
3.  [Aim of this Project](#aim-of-this-project)
4.  [Exploratory Data Analysis - EDA](#exploratory-data-analysis-EDA)
    -   [Data Loading](#data-loading)
    -   [Data Pre-processing](#data-pre-processing)
5.  [Machine Learning Model](#machine-learning-model)
    -   [Random Forest](#random-forest)
    -   [XGBoost](#xgboost)
    -   [Support Vector Machine (SVM)](#support-vector-machine-svm)
6.  [Results](#results)
    -   [Model Comparison](#model-comparison)
    -   [Confusion Matrices](#confusion-matrices)
    -   [Accuracy Comparison](#accuracy-comparison)
    -   [Spatial Predictions](#spatial-predictions)
7.  [Discussion](#discussion)
    -   [Performance Comparison](#performance-comparison)
    -   [Strengths and Weaknesses](#strengths-and-weaknesses)
    -   [Uncertainty Insights](#uncertainty-insights)
8.  [Conclusion](#conclusion)

# Acknowledgement


# Introduction 
This project demonstrates how to perform land cover classifications in R using machine learning algorithms such as Random Forests, XGBoost, and SVM, and compares their performance. This was adapted from the tutorial presented by Dr. Hanna Meyer in July 2018 and has been adapted to include additional machine learning models for comparison. The focus is on land cover classification using Sentinel satellite data, specifically targeting the invasive species **gorse (*Ulex europaeus*)** on the Banks Peninsula. This plant was brought to New Zealand during the European settlement as a hedge plant and spreads into farmland since with negative consequences for the quality of the grassland. Each year millions of dollars are spent for its control. To develop management strategies, it's of high importance to map the distribution of such invasive plants. Since field samplings mean huge expenses, remote sensing methods are required for a spatially explicit monitoring. In this tutorial, we're going to use Sentinel satellite imagery from 2017 to map the current distribution on a section of the Banks Peninsula.

# Aim of this Project 
The technical aim of the tutorial project is to show land cover classifications, based on satellite data, can be performed in R using machine learning algorithms such as Random Forests, XGBoost and SVM. We show its performance and compare the results of the models.

# Exploratory Data Analysis - EDA 
This project shows the process of land cover (LC) classification using machine learning techniques using R. The focus is on classifying land cover types, particularly the invasive species **gorse (*Ulex europaeus*)**, using Sentinel satellite data. The workflow process includes data loading, pre-processing, model training, and evaluation.

## Load required libraries
First, loading the libraries and packages that are needed for this LC classification project. The selected libraries provide functions for handling raster data, reading shapefiles, performing machine learning tasks, and visualizing results.

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Spatial and Raster data handling packages
library(raster)       # Handling Raster data 
library(sf)           # Handling Vector data 
library(terra)        # Optimise large raster datasets 
library(mapview)      # Quick raster/vector visualization

# Data Manipulation & Reshaping
library(dplyr)        # Data wrangling 
library(tidyr)        # Data reshaping
library(reshape2)     # Data reshaping (melt/cast)
library(ROSE)         # Balancing imbalanced datasets 

# Model Building & Machine Learning packages
library(caret)        # Model building
library(randomForest) # Random Forest
library(xgboost)      # XGBoost
library(e1071)        # SVM 
library(rpart)        # Recursive partitioning and Regression Trees
library(ranger)       # Faster implementaion of RF 

# Model Evaluation and Interpretation packages
library(ModelMetrics) # Performance Metrics 
library(pROC)         # ROC and AUC curves 
library(DALEX)        # Model explainability: 
library(ingredients)  # Aligns with DALEX
library(vip)          # Visualizing feature importance 


# Visualization, Plotting & Reporting packages
library(ggplot2)      # Powerful and flexible plotting 
library(RColorBrewer) # Predefined color palettes
library(rasterVis)    # Customized raster visualization
library(kableExtra)   # Table styling

```

## Data Loading and Pre-processing
### Load and explore the data
To start with, let's load the Sentinel data as well as a shapefile of the training sites.

```{r load, warning=FALSE}

# Load Sentinel data
sentinel <- stack("D:\\Study\\Machine Learning\\Geostatistics\\R-Git\\Case-Study_ML-for-Spatial-Data\\Data\\sentinel2017.grd")

# Load training sites shapefile
training <- read_sf("D:\\Study\\Machine Learning\\Geostatistics\\R-Git\\Case-Study_ML-for-Spatial-Data\\Data\\trainingSites.shp")

# Visualize the Sentinel data
print(sentinel)
print(training)

```

**Overview of the data:**
The Sentinel data is a stack of raster layers, The sentinel data subset for this tutorial contains the Sentinel channels 2-8 (visible and near infrared channels) as well as the NDVI and a yellowness index that was calculated as (red+green-blue)/(red+green+blue) as additional bands. We assume the yellowness index is valuable to distinguish the striking yellow color of the gorse from other vegetation.To get an idea about the band composition and spatial resolution, please check the Sentinel wikipedia documentation [here](https://en.wikipedia.org/wiki/Sentinel-2#Data_products). - The training sites are polygons that were digitized in QGIS on the basis of the Sentinel data using expert knowledge. The training dataset contains different land cover types, including gorse, grassland, and other vegetation types.

**Visualize the data:**
We visualize the Sentinel data as a true color composite in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.

```{r visMV, echo=TRUE, warning=FALSE, message= FALSE}

# Visualize Sentinel data with training sites
viewRGB(sentinel, r=3, g=2, b=1, map.types = "Esri.WorldImagery") + 
  mapview(training)

```

### Data Pre-processing
We need to prepare the data for machine learning by extracting raster values for the training sites and merging them with the land cover class information from the shapefile. This step is crucial for training the machine learning model. Then, we split the data into training and test datasets to ensure that we can evaluate the model's performance on unseen data.

**Extract raster information** 
In order to train the Machine Learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the spectral properties of the training sites as well as the corresponding class information. This data frame can be produced with the `extract` function. The resulting data frame contains the Sentinel data for each pixel overlayed by the polygons. This data frame then still needs to be merged with the information on the land cover class from the shapefile. This happens via the ID of the polygons which are given in the extracted data frame by the column "ID" and in the shapefile by the attribute "id".

```{r extract, echo=TRUE, warning=FALSE}

# Extract raster values for training sites
extr <- raster::extract(sentinel, training, df=TRUE)

# Merge extracted data with training sites
extr <- merge(extr, training, by.x="ID", by.y="id")
kable(head(extr), caption = "Merged Data") %>%
  kable_styling(full_width = F, position = "left")

```

**Splitting data into training and test data** 
In order to keep data for a later (nearly) independent validation as well as to limit the number of data points so that the model training won't take long time, we split the total data set into 30% training data and 70% test data. The more testing (unknown) data the higher the model accuracy. Caret's `createDataPartition` takes care that the class distribution is the same in both datasets.

```{r split, echo = TRUE, warning = FALSE}

# Set seed for reproducibility
set.seed(100)

# Create training and test dataset
trainids <- createDataPartition(extr$Class, list=FALSE, p=0.3)
trainDat <- extr[trainids,]
testDat <- extr[-trainids,]

# Ensure Class is a factor and is at the same level
trainDat$Class <- factor(trainDat$Class)
testDat$Class <- factor(testDat$Class)

# Check the distribution of classes in training and test datasets
training_class = round(prop.table(table(trainDat$Class)) * 100, 2)
test_class = round(prop.table(table(testDat$Class)) * 100, 2)

kable(training_class, caption = "Training class balance") %>%
  kable_styling(full_width = F, position = "left")
kable(test_class, caption = "Test class balance") %>%
  kable_styling(full_width = F, position = "left")

```

**Visualize relationships** 
To get an idea about the relationships between the spectral Sentinel data and the land cover class, we can visualize how the different bands differs according to the land cover class. This helps us understand the distribution of different classes in the feature space.

```{r vis_relationships, echo=TRUE, warning=FALSE}

# Visualize relationships (box plot) between yellowness index and land cover class
ggplot(trainDat, aes(x = factor(Class), y = yellowness)) +
  geom_boxplot() +
  labs(title = "Yellowness Index by Land Cover Class",
       x = "Land Cover Class",
       y = "Yellowness Index") +
  theme_minimal()

# Visualize relationships (box plot) between NDVI and land cover class
ggplot(trainDat, aes(x = factor(Class), y = NDVI)) +
  geom_boxplot() +
  labs(title = "NDVI by Land Cover Class",
       x = "Land Cover Class",
       y = "NDVI") +
  theme_minimal()

```

The box plots show the distribution of the yellowness index and NDVI for each land cover class, providing insights into how these indices vary across different classes. The Gorse class features the highest "yellowness" values while all other land cover classes have considerably lower values.

**Feature Plot**
We can also get an impression about the separability of the classes by creating a feature plot that visualizes the location of the training samples in a scatter plot of two Sentinel channels.

```{r featurePlot, echo=TRUE, warning=FALSE}

# Create feature plot to visualize relationships between Sentinel channels (B08 vs yellowness) and land cover class
ggplot(trainDat, aes(x = B08, y = yellowness, color = factor(Class))) +
  geom_point() +
  labs(title = "Feature Plot: Sentinel Band 8 vs Yellowness",
       x = "Sentinel Band 8 (NIR)",
       y = "Sentinel Band Yellowness",
       color = "Land Cover Class") +
  theme_minimal()
```

The feature plot shows the distribution of training samples in the feature space defined by Sentinel Band 8 (NIR) and the yellowness index. It helps us visualize how well the different land cover classes can be separated based on these features. The other feat Here the Gorse class features the highest "yellowness" values while all other land cover classes have considerably lower values. With view to the feature plot note that there is only low separability considering Sentinel channel 3 and 4 (green and red) but high separability when channel 8 (near infrared) or the yellowness index is included.

# Machine Learning Model 
The split data is now ready for training machine learning models. We will implement three different algorithms: Random Forest, XGBoost, and Support Vector Machine (SVM). Each model will be trained on the training dataset and evaluated on the test dataset.

**Selecting predictor and response variable**
Before building the ML models, all the Sentinel bands and indices were selected as the predictor variables and the land cover class were selected as the response variables. The response variable is categorical, while the predictor variables are continuous. From the data, we select all the information from the sentinel raster data as the predictor variables and the land cover class as the response variable.

```{r select_vars, echo=TRUE, warning=FALSE}

# Select predictor variables (Sentinel bands and indices)
predictors <- c("B02", "B03", "B04", "B05", "B06", "B07", "B08", "NDVI", "yellowness")

# Select response variable (land cover class)
response <- 'Class'

```

## Random Forest 
Random Forest is a popular ensemble learning method that constructs multiple decision trees and merges them together to get a more accurate and stable prediction. It is particularly effective for classification tasks and suits this LC classification analysis. Training the Random Forest model using the `randomForest` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

```{r random_forest, echo=TRUE, warning=FALSE}

# Ensure response is a factor
trainDat[, response] <- as.factor(trainDat[, response])

# Define hyperparameter tuning grid 
rf_grid <- expand.grid(
  mtry = c(2, 3, 4),  # Number of variables randomly sampled at each split
  splitrule = "gini", # Splitting rule
  min.node.size = c(1, 5) # Minimum size of terminal nodes
)

# Setting train control
control <- trainControl(
  method = "cv",         # k-fold CV
  number = 10,           # 10-fold
  summaryFunction = multiClassSummary,  # for multiclass metrics
  classProbs = TRUE,
  savePredictions = "final",
  sampling = "up"
)

# Train Random Forest model
set.seed(100)
model_rf <- train(trainDat[, predictors], trainDat[, response], 
                  method = "rf", 
                  trControl = control,
                  importance = TRUE)

```

The Random Forest model is successfully trained and variable importance is noted for the determining which predictor variable (sentinel bands) has overall contributions to the classification.

```{r rf_summary, echo=TRUE, message=FALSE}

# Print model summary
kable(model_rf$results, caption = "Random Forest Model Results") %>%
  kable_styling(full_width = F, position = "left")

# Extract variable importance
importance_df <- varImp(model_rf)$importance

# Add variable names as a column
importance_df$Variable <- rownames(importance_df)

# Compute overall importance (if multiple classes)
importance_df$Overall <- rowMeans(importance_df[, sapply(importance_df, is.numeric)])

# Round and sort in descending order
importance_df <- importance_df %>%
  mutate(Overall = round(Overall, 3)) %>%
  arrange(desc(Overall))  # High to low

# View top variables
kable(importance_df, caption = "Variable Importance for Random Forest Model") %>%
  kable_styling(full_width = F, position = "left")
  
# Plot variable importance
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance for RF model (High to Low)",
       x = "Variable", y = "Importance Score") +
  theme_minimal()

```

The Random Forest model has been retrained successfully with the selected variables. The model achieved excellent performance across multiple values of `mtry`, with the least logLoss results at 5. At `mtry` at 5, the model shows, 
  - the least uncertainity (*logLoss = 0.0192*), 
  - nearly perfect classification (*AUC score = 0.999*), 
  - very high classification (*accuracy = 0.9933*), 
  - good overall class performance (*balanced accuracy = 0.9831*) and 
  - a balanced precision and recall (*mean F1 score = 0.968*).

The variable importance scores for each predictor variable is derived. The variable importance plot shows which features contribute most to the classification task, with the `B05`,`NDVI` & `Yellowness` being among the top 3 important predictors.

**Predicting the test data with confusion matrix and scoring metrics** 
To evaluate the performance of the Random Forest model on the test dataset, we can create a confusion matrix and calculate scoring metrics such as accuracy, sensitivity, and specificity. The confusion matrix provides insights into how well the model performs on the test dataset.

```{r rf_confusion_matrix, echo=TRUE, warning=FALSE, message=FALSE}

# Predict on the test dataset
predictions_rf <- predict(model_rf,
                          newdata = testDat[, predictors])

# Extract the levels from the training data response
true_levels <- levels(trainDat[[response]])

# Convert both predicted and actual responses to factors with the same levels
predictions_rf <- factor(predictions_rf, levels = true_levels)
actual_rf <- factor(testDat[[response]], levels = true_levels)

# Construct the confusion matrix
confusion_rf <- caret::confusionMatrix(predictions_rf, actual_rf)

# Plot confusion matrix
cm_plot_rf <- ggplot(as.data.frame(confusion_rf$table), 
                     aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix for Random Forest Model",
       x = "Reference Class",
       y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

cm_plot_rf

# Print scoring metrics
kable(confusion_rf$overall, caption = "Scoring Metrics for Random Forest Model") %>%
  kable_styling(full_width = F, position = "left")

# Print sensitivity and specificity for each class
kable(confusion_rf$byClass, caption = "Sensitivity and Specificity for Random Forest Model") %>%
  kable_styling(full_width = F, position = "left")

```

**Results**
Confusion Matrix and scoring metrics provide insights into the performance of the Random Forest model on the test dataset. The confusion matrix shows how many instances were correctly classified for each land cover class, while the scoring metrics (accuracy, sensitivity, specificity) quantify the model's overall performance. 
Predictions on the test/unseen data, confusion matrix shows high true positives for all the classes predictions with an overall accuracy of 99.56%, but do have some irregularities in the classification. From the confusion matrix, 
  - `Water`, `Grassland`, `Gorse`, & `Bush` are 100% predicted and classfied correctly. 
  - Out of 738 `Forestry` Class, 1 was predicted as Bush
  - Out of 182 `Urban` Class, 10 were predicted as `Sand` and 11 as `Bare`.
  - Out of 221 `Sand` Class, 12 were predicted as `Urban`. 
  - Out of 455 `Bare` class, 6 were predicted as `Urban`.

This means that the `Urban`, `Bare` and `Sand` are very little false positives with the RF model.

## XGBoost 
XGBoost (Extreme Gradient Boosting) is a powerful and efficient implementation of gradient boosting that is widely used for classification tasks. It is known for its speed and performance, making it suitable for large datasets. Training the XGBoost model using the `xgboost` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

```{r xgboost, echo=TRUE, warning=FALSE, message=FALSE}

# Ensure response is a factor
trainDat[, response] <- as.factor(trainDat[, response])

# Define hyperparameter tuning grid 
xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Setting train control
control <- trainControl(
  method = "cv",         # k-fold CV
  number = 10,           # 10-fold
  summaryFunction = multiClassSummary,  # for multiclass metrics
  classProbs = TRUE,
  savePredictions = "final",
  sampling = "up",
  verboseIter = FALSE
)

# Train the XGBoost model using caret
set.seed(100)
model_xgb <- train(x = trainDat[, predictors],
                   y = trainDat[, response],
                   method = "xgbTree",
                   trControl = control,
                   tuneGrid = xgb_grid)
  
```

The XGBoost model is successfully trained and variable importance is noted for the determining which predictor variable (sentinel bands) has overall contributions to the classification.

```{r xgb_summary, echo=TRUE, message=FALSE}

# Print model summary
kable(model_xgb$results, caption = "XGBoost Model Results") %>%
  kable_styling(full_width = F, position = "left")

# Extract variable importance
importance_xgb <- xgb.importance(feature_names = predictors, 
                                 model = model_xgb$finalModel)

# Convert importance to a data frame
importance_df_xgb <- as.data.frame(importance_xgb)
importance_df_xgb <- importance_df_xgb %>%
  mutate(Variable = reorder(Feature, Gain)) %>%
  arrange(desc(Gain))

# Check the summary of the importance data frame
kable(importance_df_xgb, caption = "Variable Importance for XGBoost Model") %>%
  kable_styling(full_width = F, position = "left")

# Visualize variable importance using ggplot2
ggplot(importance_df_xgb, aes(x = Variable, y = Gain)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance for XGBoost (High to Low)",
       x = "Variable", y = "Importance Score") +
  theme_minimal()

```

The variable importance scores for each predictor variable is derived. The variable importance plot shows which features contribute most to the classification task, with the `B05`, `Yellowness` & `B02` being among the top 3 important predictors for the XGBoost model.

The XGBoost model has been trained successfully with the selected hyperparameter. The model achieved excellent performance having, 
  - the least uncertainity (*logLoss = 0.0283*), 
  - nearly perfect classification (*AUC score = 0.999*), 
  - perfect aggrement beyond chance (*Kappa = 0.989*), 
  - very high classification (*Accuracy = 0.993*), 
  - good overall class performance (*balanced accuracy = 0.984*), and 
  - a balanced precision and recall (*mean F1 score = 0.97*). 
These score are very similar to the Random Forest Model and will be compared in the results and discussion section.

**Predicting the test data with confusion matrix and scoring metrics** To evaluate the performance of the XGBoost model on the test dataset, we can create a confusion matrix and calculate scoring metrics such as accuracy, sensitivity, and specificity. The confusion matrix provides insights into how well the model performs on the test dataset.

```{r xgb_confusion_matrix}

# Predict on the test dataset
predictions_xgb <- predict(model_xgb, newdata = testDat[, predictors])

# Extract the levels from the training data response
true_levels_xgb <- levels(trainDat[[response]])

# Convert both predicted and actual responses to factors with the same levels
predictions_xgb <- factor(predictions_xgb, levels = true_levels_xgb)
actual_xgb <- factor(testDat[[response]], levels = true_levels_xgb)

# Now create the confusion matrix
confusion_xgb <- caret::confusionMatrix(predictions_xgb, actual_xgb)

# Plot confusion matrix
cm_plot_xgb <- ggplot(as.data.frame(confusion_xgb$table), aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix for XGBoost Model",
       x = "Reference Class",
       y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

cm_plot_xgb

# Print scoring metrics
kable(confusion_xgb$overall, caption = "Scoring Metrics for XGBoost Model") %>% 
  kable_styling(full_width = F, position = "left")

# Print sensitivity and specificity for each class
kable(confusion_xgb$byClass, caption = "Sensitivity and Specificity for XGBoost Model") %>% 
  kable_styling(full_width = F, position = "left")

```

**Results** 
Confusion Matrix and scoring metrics provide insights into the performance of the XGBoost model on the test dataset. The confusion matrix shows how many instances were correctly classified for each land cover class, while the scoring metrics (accuracy, sensitivity, specificity) quantify the model's overall performance.

Predictions on the test/unseen data for the XGBoost model, confusion matrix shows high true positives for all the classes predictions with an overall accuracy of 99.27%, but do have some irregularities in the classification. From the confusion matrix, 
  - `Water`, `Grassland`, & `Forestry` are 100% predicted and classfied correctly. 
  - Out of 182 `Urban` Class, 15 were predicted as `Sand` and 11 as `Bare` 
  - Out of 221 `Sand` Class, 18 were predicted as `Urban` 
  - Out of 634 `Gorse` Class, 2 were predicted as `Grassland` and 1 as Urban
  - Out of 733 `Bush` Class, 1 was predicted as `Water` 
  - Out of 455 `Bare` class, 7 were predicted as `Urban` and 1 as `Bush`.

This means that the `Urban`, `Bare`, `Bush`, `Gorse` and `Sand` are hold little false positives with the XGBoost model.

## Support Vector Machine (SVM)
Support Vector Machine (SVM) is a supervised learning algorithm that can be used for classification tasks. It works by finding the hyperplane that best separates the classes in the feature space. SVM is effective in high-dimensional spaces and is suitable for this LC classification analysis. Training the SVM model using the `e1071` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

```{r svm, echo=TRUE, warning=FALSE}

# Ensure response is a factor and class weights are a list
trainDat[, response] <- as.factor(trainDat[, response])

# Define hyperparameter tuning grid 
svm_grid <- expand.grid(
  C = 2^(-1:1),         # Cost parameter
  sigma = 2^(-1:1))     # Kernel parameter

# Setting train control
control <- trainControl(
  method = "cv",         # k-fold CV
  number = 10,           # 10-fold
  summaryFunction = multiClassSummary,
  classProbs = TRUE,
  savePredictions = "final",
  sampling = "up")

# Train Support Vector Machine (SVM) model
set.seed(100)
model_svm <- train(x = trainDat[, predictors],
                   y = trainDat[, response],
                   method = "svmRadial",
                   trControl = control,
                   tuneGrid = svm_grid)

```

The SVM model is successfully trained, summary results and variable importance are noted for the determining which predictor variable (sentinel bands) has overall contributions to this classification.

```{r svm_summary, echo=TRUE, message=FALSE}

# Print model summary
kable(model_svm$results, caption = "Support Vector Machine Model Results") %>%
  kable_styling(full_width = F, position = "left")

# Extract variable importance
importance_svm <- varImp(model_svm)$importance

# Add variable names as a column
importance_svm$Variable <- rownames(importance_svm)

# Compute overall importance (if multiple classes)
importance_svm$Overall <- rowMeans(importance_svm[, sapply(importance_svm, is.numeric)])

# Round and sort in descending order
importance_svm <- importance_svm %>%
  mutate(Overall = round(Overall, 3)) %>%
  arrange(desc(Overall))  # High to low

# Print top variables
kable(importance_svm, caption = "Variable Importance for Support Vector Machine Model") %>%
  kable_styling(full_width = F, position = "left")

# Plot variable importance
ggplot(importance_svm, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance for SVM model (High to Low)",
       x = "Variable", y = "Importance Score") +
  theme_minimal()

```

The variable importance scores for each predictor variable is derived. The variable importance plot shows which features contribute most to the classification task, with the `NDVI`, `B04` & `Yellowness` being among the top 3 important predictors for the XGBoost model.

The SVM model has not been trained successfully because the model achieved poor performance having very low scores for the different scoring matrix. 

**Predicting the test data with confusion matrix and scoring metrics** To evaluate the performance of the SVM model on the test dataset, we can create a confusion matrix and calculate scoring metrics such as accuracy, sensitivity, and specificity. The confusion matrix provides insights into how well the model performs on the test dataset.

```{r svm_confusion_matrix, warning=FALSE, message=FALSE}

# Predict on the test dataset
predictions_svm <- predict(model_svm, 
                           newdata = testDat[, predictors])
probs_svm <- predict(model_svm, 
                     newdata = testDat[, predictors],
                     probability = TRUE)

# Extract the levels from the training data response
true_levels_svm <- levels(trainDat[[response]])

# Convert both predicted and actual responses to factors with the same levels
predictions_svm <- factor(predictions_svm, 
                          levels = true_levels_svm)
actual_svm <- factor(testDat[[response]], 
                     levels = true_levels_svm)

# Construct the confusion matrix
confusion_svm <- caret::confusionMatrix(predictions_svm, actual_svm)

# Plot confusion matrix
cm_plot_svm <- ggplot(as.data.frame(confusion_svm$table), aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix for Support Vector Machine Model",
       x = "Reference Class",
       y = "Predicted Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot Confusion Matrix
cm_plot_svm

# Print scoring metrics
kable(confusion_svm$overall, caption = "Scoring Metrics for Support Vector Machine Model") %>%
  kable_styling(full_width = F, position = "left")

# Print sensitivity and specificity for each class
kable(confusion_svm$byClass, caption = "Sensitivity and Specificity for Support Vector Machine Model") %>%
  kable_styling(full_width = F, position = "left")

```

**Results** 
Confusion Matrix and scoring metrics provide insights into the performance of the Support Vector Machine (SVM) model on the test dataset. The confusion matrix shows how many instances were correctly classified for each land cover class, while the scoring metrics (accuracy, sensitivity, specificity) quantify the model's overall performance.

In the SVM model, the confusion matrix shows that the model has not classified the defined land cover classes as there are lot of misclassification. Hence the model does not fit this land cover classification and will not be further used to compare the models and assess the results and discussions of this project.

# Results & Discussions 
## Confusion matrices & Scoring metrics
Comparing the Confusion matrices of the models, we can see how well each model performed in classifying the land cover types. The confusion matrices show the number of true positives, false positives, true negatives, and false negatives for each class. This allows us to assess the strengths and weaknesses of each model in terms of classification accuracy.

```{r confusion_matrices, message=FALSE}

# Create a list of confusion matrices for each model
cm_plot_rf
cm_plot_xgb

```

Compare the scoring metrics (precision, f1score, recall, accuracy, sensitivity, specificity) of the three models to see which model performed best overall. The accuracy metric indicates the overall correctness of the model, while sensitivity and specificity provide insights into how well the model identifies each class.

```{r scoring_metrics, message=FALSE}

# Create a data frame to hold the scoring metrics for each model from the test dataset
scoring_df <- data.frame(
  Model = c("Random Forest", "XGBoost"),
  Accuracy = c(
    confusion_rf$overall['Accuracy'],
    confusion_xgb$overall['Accuracy']),
  Kappa = c(
    confusion_rf$overall['Kappa'], 
    confusion_xgb$overall['Kappa']),
  Precision = c(
    mean(confusion_rf$byClass[,"Precision"]), 
    mean(confusion_xgb$byClass[,"Precision"])),
  Sensitivity = c(
    mean(confusion_rf$byClass[,"Sensitivity"]),
    mean(confusion_xgb$byClass[,"Sensitivity"])),
  Specificity = c(
    mean(confusion_rf$byClass[,"Specificity"]),
    mean(confusion_xgb$byClass[,"Specificity"])),
  F1_Score = c(
    mean(confusion_rf$byClass[,"F1"]), 
    mean(confusion_xgb$byClass[,"F1"]))
)

# Print the scoring metrics data frame
kable(scoring_df, caption = "Scoring Metrics for Each Model") %>%
  kable_styling(full_width = F, position = "left")

# Set the order for the plot
scoring_df$Model <- factor(scoring_df$Model,
                           levels = 
                             c('Random Forest',
                               'XGBoost'))

# Plot the scoring metrics for each model
scoring_df_long <- melt(scoring_df, id.vars = "Model")
ggplot(scoring_df_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Scoring Metrics Comparison between the models",
       x = "Model", y = "Score") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

``` {r model performance metrics}
# Convert list to tibble
measures_to_df <- function(measures, model_name) {
  as_tibble(measures, .name_repair = "minimal") %>%
    pivot_longer(cols = everything(), names_to = "metric", 
                 values_to = "value") %>%
    mutate(model = model_name)
}

# Apply to each model
rf_measures  <- measures_to_df(mp_rf$measures, "Random Forest")
xgb_measures <- measures_to_df(mp_xgb$measures, "XGBoost")

# Combine all
all_measures <- bind_rows(rf_measures, xgb_measures)

# Reshape: one row per measure, one column per model
measures_wide <- all_measures %>%
  pivot_wider(names_from = model, values_from = value)

kable(measures_wide, caption = "Metric measurement for Residual performance") %>%
  kable_styling(full_width = F, position = "left")

```

**Discussion - Model Performance**
Confusion matrix from Random Forest and XGBoost are shown here. As per the results from the confusion matrix, the Random Forest Model scores high true positives and very low false positives, when compared with the XGBoost model. Furthermore, the scoring metrics provide a comprehensive overview of the performance of each model. The Random Forest model achieved the highest accuracy compared to the XGBoost. The precision and sensitivity scores indicate that Random Forest performed well in identifying the gorse class than the XGBoost. The F1 score, which balances precision and recall, also shows that Random Forest outperformed XGBoost.

Hence, Based on this, Random Forest Model outperforms XGBoost in predicting the invasive species **gorse (*Ulex europaeus*)**, using Sentinel satellite data, on the Banks Peninsula of New Zealand. 

## Uncertainly Analysis
**1. DALEX Uncertainity Analysis** To gain insights into the uncertainty of the model predictions, we can use the DALEX package to create an explainer for each model. This allows us to analyze the model's predictions and understand how different features contribute to the uncertainty in the predictions.

```{r uncertainty_analysis, message=FALSE}

# Define a safe predict function
predict_class <- function(m, d) predict(m, d, type = "raw")

# Create explainers for each model using DALEX
explainer_rf <- DALEX::explain(model_rf, 
                               data = testDat[, predictors], 
                               y = testDat$Class, 
                               label = "Random Forest")
explainer_xgb <- DALEX::explain(model_xgb, 
                                data = testDat[, predictors], 
                                y = testDat$Class, 
                                label = "XGBoost")

# Residual performance (classification)
mp_rf <- model_performance(explainer_rf)
mp_xgb <- model_performance(explainer_xgb)

# Plot ROC or accuracy
plot(mp_rf)
plot(mp_xgb)

```
**2. Probability-based Entropy Maps (for uncertainty)**
For this uncertainity analysis, first predicting the class probabilities for the models is required. With the predicted class probabilities, entropy per pixel is calculated and then map these entropy values as a raster data. 

```{r entropy scores}

# Preparing the data
# Convert the raster predictors to dataframe
pred_data <- as.data.frame(predictors_stack, na.rm = TRUE)


# Predicting the model probabilities
prob_rf <- predict(model_rf, pred_data, type='prob')
prob_rf
# Convert pred_data to matrix for XGBoost
dtest <- xgb.DMatrix(data = as.matrix(pred_data))
prob_xgb <- predict(model_xgb, dtest)
# Reshaping the prob into a matrix 
prob_xgb <- matrix(prob_xgb, ncol=length(classes), byrow=TRUE)
colnames(prob_xgb) <- classes

# Computing the entropy for the models
compute_entropy <- function (p) {
  epsilon <- 1e-12 # Avoid log 0
  p <- p - epsilon 
  -rowSums(p * log2(p))
}

# Apply the model probabilitites to the entropy function
entropy_rf <- compute_entropy(as.matrix(prob_rf))
entropy_xgb <- compute_entropy(as.matrix(prob_xgb))

# Setting up the plot
# Create templates for the model
entropy_raster_rf <- predictors.stack[[1]]
entropy_raster_xgb <- predictors.stack[[1]]
# Setting all values to NA
values(entropy_raster_rf) <- NA
values(entropy_raster_xgb) <- NA
# Inserting the entropy values at valid locations
values(entropy_raster_rf)[!is.na(rowSums(pred_data))] <- entropy_rf
values(entropy_raster_xgb)[!is.na(rowSums(pred_data))] <- entropy_xgb

# Plotting the entropy raster
plot(entropy_raster_rf,  main = "Random Forest Entropy", 
     col = viridis(100))
plot(entropy_raster_xgb, main = "XGBoost Entropy",       
     col = viridis(100))                               
                               

```

## Predicted maps
Now the predicted model maps are visualized to check the land cover classification and lay the training classes on the predicted models to verify the classification visually. 
```{r stacking sentinel}

# Stacking Sentinel bands + indices
predictors_stack <- stack(sentinel$B02, 
                          sentinel$B03, 
                          sentinel$B04, 
                          sentinel$B05,
                          sentinel$B06, 
                          sentinel$B07, 
                          sentinel$B08,
                          sentinel$NDVI, 
                          sentinel$yellowness)

# Convert raster stack to dataframe (pixel wise)
pixels_df <- as.data.frame(predictors_stack, na.rm = TRUE)

# Predict for raster cells for each model
predictions_rf <- predict(model_rf, newdata = pixels_df, type = "raw")
predictions_xbg <- predict(model_xgb, newdata = pixels_df, type = "raw")

```

```{r predicted maps, echo=TRUE, warning=FALSE, message=FALSE}

# Create raster layers for each model prediction
raster_rf  <- setValues(raster(predictors_stack), predictions_rf)
raster_xgb <- setValues(raster(predictors_stack), predictions_xbg)

# Define class names and palette
classes <- c("Bare", "Bush", "Forestry", "Gorse", "Grassland", "Sand", "Urban", "Water")# Replace with your actual column name
class_colors <- c(
  "Bare"       = "#a6643a",   # brown
  "Bush"       = "#d01c8b",   # magenta
  "Forestry"   = "#1b7837",   # dark green
  "Gorse"      = "#ffff00",   # yellow
  "Grassland"  = "#4dac26",   # light green
  "Sand"       = "#f6e8c3",   # sand yellow
  "Urban"      = "#878787",   # gray
  "Water"      = "#4393c3"    # blue
)
palette <- unname(class_colors[classes])

# Convert to SpatRaster (terra) and assign class levels
raster_rf  <- rast(raster_rf);  levels(raster_rf)  <- data.frame(value = 1:8, class = classes)

raster_xgb <- rast(raster_xgb); levels(raster_xgb) <- data.frame(value = 1:8, class = classes)

mapview(raster_rf, col.regions = palette, na.color = NA) + 
  mapview(training, zcol = "Class", col.regions = "black", alpha = 0.5)

mapview(raster_xgb, col.regions = palette, na.color = NA) + 
  mapview(training, zcol = "Class", col.regions = "black", alpha = 0.5)

```

**Discussions** 
The predicted thematic maps from the Models shows atmost accurate calssification as indicated by the confusion matrix and scoring metrics. 

# Conclusion


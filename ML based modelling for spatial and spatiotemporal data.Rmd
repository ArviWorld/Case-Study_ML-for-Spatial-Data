---
title:  "Land cover classification with ML in R"
subtitle: "Example of the Banks Peninsula in New Zealand"
author: "Dr. Hanna Meyer"
worked_by: "Aravindh Venkatraman"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 6
  word_document: 
    toc: yes
    toc_depth: '6'
  html_document: default
  always_allow_html: yes
---

# Table of Contents

-   [Introduction](#introduction)
-   [Aim of this Project](#aim-of-this-project)
-   EDA
    -   [Data Loading](#data-loading)
    -   [Data Pre-processing](#data-pre-processing)
-   [Machine Learning Model](#machine-learning-model)
    -   [Random Forest](#random-forest)
    -   [XGBoost](#xgboost)
    -   [Support Vector Machine (SVM)](#support-vector-machine-svm)
-   [Results](#results)
    -   [Model Comparison](#model-comparison)
    -   [Confusion Matrices](#confusion-matrices)
    -   [Accuracy Comparison](#accuracy-comparison)
    -   [Spatial Predictions](#spatial-predictions)
-   [Discussion](#discussion)
    -   [Performance Comparison](#performance-comparison)
    -   [Strengths and Weaknesses](#strengths-and-weaknesses)
    -   [Uncertainty Insights](#uncertainty-insights)

# Introduction {#introduction}

The tutorial project demonstrates how to perform land cover classifications in R using machine learning algorithms such as Random Forests, XGBoost, and SVM, and compares their performance. This tutorial was presented by Dr. Hanna Meyer in July 2018. This tutorial has been adapted by Aravindh Venkatraman to include additional machine learning models for comparison. The focus is on land cover classification using Sentinel satellite data, specifically targeting the invasive species **gorse (*Ulex europaeus*)** on the Banks Peninsula. This plant was brought to New Zealand during the European settlement as a hedge plant and spreads into farmland since with negative consequences for the quality of the grassland. Each year millions of dollars are spent for its control. To develop management strategies, it's of high importance to map the distribution of such invasive plants. Since field samplings mean huge expenses, remote sensing methods are required for a spatially explicit monitoring. In this tutorial, we're going to use Sentinel satellite imagery from 2017 to map the current distribution on a section of the Banks Peninsula.

# Aim of this Project {#aim-of-this-project}

The technical aim of the tutorial project is to show land cover classifications, based on satellite data, can be performed in R using machine learning algorithms such as Random Forests, XGBoost and SVM. We show its performance and compare the results of the models.

# Methods {#methods}

This tutorial will guide you through the process of land cover (LC) classification using machine learning techniques in R. The focus is on classifying land cover types, particularly the invasive species **gorse (*Ulex europaeus*)**, using Sentinel satellite data. The steps include data loading, pre-processing, model training, and evaluation.

## Load required libraries

We select and load the libraries and packages that are needed for this LC classification project. The selected libraries provide functions for handling raster data, reading shapefiles, performing machine learning tasks, and visualizing results.

```{r setup, message=FALSE, warning=FALSE}

library(raster)
library(caret)
library(mapview)
library(sf)
library(randomForest)
library(xgboost)
library(e1071)
library(ggplot2)
library(dplyr)
library(rpart)
library(ranger)
library(terra)
library(DALEX)
library(vip)
library(pROC)
library(kableExtra)

```

## Data Loading and Pre-processing

### Load and explore the data

To start with, let's load the Sentinel data as well as a shapefile of the training sites.

```{r load}

# Load Sentinel data
sentinel <- stack("D:\\Study\\Machine Learning\\Geostatistics\\R-Git\\Case-Study_ML-for-Spatial-Data\\Data\\sentinel2017.grd")

# Load training sites shapefile
training <- read_sf("D:\\Study\\Machine Learning\\Geostatistics\\R-Git\\Case-Study_ML-for-Spatial-Data\\Data\\trainingSites.shp")

# Visualize the Sentinel data
print(sentinel)
print(training)

```

**Overview of the data:** - The Sentinel data is a stack of raster layers, The sentinel data subset for this tutorial contains the Sentinel channels 2-8 (visible and near infrared channels) as well as the NDVI and a yellowness index that was calculated as (red+green-blue)/(red+green+blue) as additional bands. We assume the yellowness index is valuable to distinguish the striking yellow color of the gorse from other vegetation.To get an idea about the band composition and spatial resolution, please check the Sentinel wikipedia documentation [here](https://en.wikipedia.org/wiki/Sentinel-2#Data_products). - The training sites are polygons that were digitized in QGIS on the basis of the Sentinel data using expert knowledge. The training dataset contains different land cover types, including gorse, grassland, and other vegetation types.

**Visualize the data:** We visualize the Sentinel data as a true color composite in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.

```{r visMV, warning=FALSE, message= FALSE}

# Visualize Sentinel data with training sites
viewRGB(sentinel, r = 3, g = 2, b = 1, map.types = "Esri.WorldImagery") +
  mapview(training)

```

### Data Pre-processing {#data-pre-processing}

We need to prepare the data for machine learning by extracting raster values for the training sites and merging them with the land cover class information from the shapefile. This step is crucial for training the machine learning model. Then, we split the data into training and test datasets to ensure that we can evaluate the model's performance on unseen data.

**Extract raster information** In order to train the Machine Learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the spectral properties of the training sites as well as the corresponding class information. This data frame can be produced with the `extract` function. The resulting data frame contains the Sentinel data for each pixel overlayed by the polygons. This data frame then still needs to be merged with the information on the land cover class from the shapefile. This happens via the ID of the polygons which are given in the extracted data frame by the column "ID" and in the shapefile by the attribute "id".

```{r extract}

# Extract raster values for training sites
extr <- extract(sentinel, training, df=TRUE)

# Merge extracted data with training sites
extr <- merge(extr, training, by.x="ID", by.y="id")
head(extr)

```

**Split data** In order to keep data for a later (nearly) independent validation as well as to limit the number of data points so that the model training won't take long time, we split the total data set into 30% training data and 70% test data. Caret's `createDataPartition` takes care that the class distribution is the same in both datasets. We put the test data to the side and first only continue with the training data.

```{r split}

# Set seed for reproducibility
set.seed(100)

# Create training and test datasets
trainids <- createDataPartition(extr$Class, list=FALSE, p=0.3)
trainDat <- extr[trainids,]
testDat <- extr[-trainids,]

```

**Visualize relationships** To get an idea about the relationships between the spectral Sentinel data and the land cover class, we can visualize how the different bands differs according to the land cover class. This helps us understand the distribution of different classes in the feature space.

```{r vis_relationships}

# Visualize relationships (box plot) between yellowness index and land cover class
ggplot(trainDat, aes(x = factor(Class), y = yellowness)) +
  geom_boxplot() +
  labs(title = "Yellowness Index by Land Cover Class",
       x = "Land Cover Class",
       y = "Yellowness Index") +
  theme_minimal()

# Visualize relationships (box plot) between NDVI and land cover class
ggplot(trainDat, aes(x = factor(Class), y = NDVI)) +
  geom_boxplot() +
  labs(title = "NDVI by Land Cover Class",
       x = "Land Cover Class",
       y = "NDVI") +
  theme_minimal()

```

The box plots show the distribution of the yellowness index and NDVI for each land cover class, providing insights into how these indices vary across different classes. The Gorse class features the highest "yellowness" values while all other land cover classes have considerably lower values.

**Feature Plot** We can also get an impression about the separability of the classes by creating a feature plot that visualizes the location of the training samples in a scatter plot of two Sentinel channels.

```{r featurePlot}

# Create feature plot to visualize relationships between Sentinel channels (B08 vs yellowness) and land cover class
ggplot(trainDat, aes(x = B08, y = yellowness, color = factor(Class))) +
  geom_point() +
  labs(title = "Feature Plot: Sentinel Band 8 vs Yellowness",
       x = "Sentinel Band 8 (NIR)",
       y = "Sentinel Band Yellowness",
       color = "Land Cover Class") +
  theme_minimal()
```

The feature plot shows the distribution of training samples in the feature space defined by Sentinel Band 8 (NIR) and the yellowness index. It helps us visualize how well the different land cover classes can be separated based on these features. The other feat Here the Gorse class features the highest "yellowness" values while all other land cover classes have considerably lower values. With view to the feature plot note that there is only low separability considering Sentinel channel 3 and 4 (green and red) but high separability when channel 8 (near infrared) or the yellowness index is included.

# Machine Learning Model {#machine-learning-model}

The split data is now ready for training machine learning models. We will implement three different algorithms: Random Forest, XGBoost, and Support Vector Machine (SVM). Each model will be trained on the training dataset and evaluated on the test dataset.

**Selecting predictor and response variable** Before building the ML models, we need to select the predictor variables (the Sentinel bands and indices) and the response variable (the land cover class). The response variable is categorical, while the predictor variables are continuous. From the data, we select all the information from the sentinel raster data as the predictor variables and the land cover class as the response variable.

```{r select_vars}

# Select predictor variables (Sentinel bands and indices)
predictors <- c("B02", "B03", "B04", "B05", "B06", "B07", "B08", "NDVI", "yellowness")

# Select response variable (land cover class)
response <- 'Class'

```

## Random Forest {#random-forest}

Random Forest is a popular ensemble learning method that constructs multiple decision trees and merges them together to get a more accurate and stable prediction. It is particularly effective for classification tasks and suits this LC classification analysis.

Training the Random Forest model using the `randomForest` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

```{r random_forest}

# Train Random Forest model
set.seed(100)
model_rf <- train(trainDat[, predictors], trainDat[, response], 
                  method = "rf", 
                  trControl = trainControl(method = "cv", number = 10), 
                  importance = TRUE)

```

```{r rf_summary, echo=FALSE, message=FALSE}

# Print model summary
kable(model_rf$results, caption = "Random Forest Model Results") %>%
  kable_styling(full_width = F, position = "left")

# Extract variable importance
importance_df <- varImp(model_rf)$importance

# Add variable names as a column
importance_df$Variable <- rownames(importance_df)

# Compute overall importance (if multiple classes)
importance_df$Overall <- rowMeans(importance_df[, sapply(importance_df, is.numeric)])

# Round and sort in descending order
importance_df <- importance_df %>%
  mutate(Overall = round(Overall, 3)) %>%
  arrange(desc(Overall))  # High to low

# Check the summary of the importance data frame
kable(importance_df, caption = "Variable Importance Summary") %>%
  kable_styling(full_width = F, position = "left")

# View top variables
kable(importance_df, caption = "Variable Importance for Random Forest Model") %>%
  kable_styling(full_width = F, position = "left")
  
# Plot variable importance
ggplot(importance_df, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (High to Low)",
       x = "Variable", y = "Importance Score") +
  theme_minimal()

```

## XGBoost {#xgboost}

XGBoost (Extreme Gradient Boosting) is a powerful and efficient implementation of gradient boosting that is widely used for classification tasks. It is known for its speed and performance, making it suitable for large datasets. Training the XGBoost model using the `xgboost` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

``` {r xgboost}
# Ensure response is a factor
trainDat[, response] <- as.factor(trainDat[, response])

# Set training control
control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

# Train the XGBoost model using caret
set.seed(123)
model_xgb <- train(x = trainDat[, predictors],
                   y = trainDat[, response],
                   method = "xgbTree",
                   trControl = control)
```

``` {r xgb_summary, echo=TRUE, message=FALSE}

# Print model summary
kable(model_xgb$results, caption = "XGBoost Model Results") %>%
  kable_styling(full_width = F, position = "left")

# Extract variable importance
importance_xgb <- xgb.importance(feature_names = predictors, model = model_xgb$finalModel)

# Convert importance to a data frame
importance_df_xgb <- as.data.frame(importance_xgb)
importance_df_xgb <- importance_df_xgb %>%
  mutate(Variable = reorder(Feature, Gain)) %>%
  arrange(desc(Gain))

# Check the summary of the importance data frame
kable(importance_df_xgb, caption = "Variable Importance for XGBoost Model") %>%
  kable_styling(full_width = F, position = "left")

# View top variables
kable(head(importance_df_xgb, 10), caption = "Top 10 Variables for XGBoost Model") %>%
  kable_styling(full_width = F, position = "left")

# Visualize variable importance
vip::vip(model_xgb, num_features = 10) +
  labs(title = "Variable Importance for XGBoost Model")

# Visualize variable importance using ggplot2
ggplot(model_xgb$results, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance for XGBoost Model",
       x = "Variables",
       y = "Importance") +
  theme_minimal()


```

## Support Vector Machine (SVM) {#support-vector-machine-svm}

Support Vector Machine (SVM) is a supervised learning algorithm that can be used for classification tasks. It works by finding the hyperplane that best separates the classes in the feature space. SVM is effective in high-dimensional spaces and is suitable for this LC classification analysis. Training the SVM model using the `e1071` package in R. The model is trained on the training dataset, and we can visualize the variable importance to understand which features contribute most to the classification. We use the `train` function from the `caret` package to train the model with 10-fold cross-validation for hyperparameter tuning.

```{r svm}

# Train Support Vector Machine (SVM) model
set.seed(100)
model_svm <- train(trainDat[, predictors], trainDat[, response], 
                   method = "svmRadial", 
                   trControl = trainControl(method = "cv", number = 10), 
                   importance = TRUE)

# Print model summary
print(model_svm)

```

**Results** The trained models are now ready for evaluation. We will compare the performance of the Random Forest, XGBoost, and SVM models using confusion matrices, accuracy comparison plots, and spatial predictions.

## Methods

ML algorithms, spatial prediction.

## Results {#results}

Confusion matrices, accuracy comparison plots, maps.

## Discussion {#discussion}

Performance comparison, strengths/weaknesses, uncertainty insights.
